---
title: Network - 웹 로봇
date: 2019-10-20 23:59:45
tags: [HTTP, 웹 로봇, network]
---

![images](/images/network/network.jpg)<br/>

# 웹 로봇
웹 로봇은 사람과의 상호작용 없이 연속된 웹 트랜잭션들을 `자동`으로 수행하는 소프트웨어 프로그램입니다. 자동으로 웹 사이트를 돌아다니면서 컨텐츠를 가져오고, 하이퍼링크를 따라가고, 그들의 발견한 데이터를 처리합니다.<br/>
<br/>

## 크롤러와 크롤링
`웹 크롤러`는, 먼저 웹페이지를 한 개 가져오고, 그 다음 그 페이지가 가리키는 모든 웹페이지를 가져오고, 다시 그 페이지들이 가리키는 모든 웹페이지들을 가져오는, 이러한 일을 **재귀적**으로 반복하는 방식으로 웹을 순회하는 로봇입니다.<br/>

### 어디에서 시작하는가: '루트 집합'
크롤러가 방문을 시작하는 URL들의 초기 집합은 `루트 집합`이라고 불립니다. 일반적으로 `좋은 루트 집합`은 크고 인기 있는 **웹 사이트**, **새로 생성된 페이지들의 목록**, 그리고 **자주 링크되지 않는 잘 알려져 있지 않은 페이지들의 목록**으로 구성되어 있습니다.<br/>
<br/>

### 링크 추출과 상대 링크 정상화 
크롤러는 웹을 돌아다니면서 꾸준히 HTML 문서를 검색하는데 이 때 각 페이지 안에 들어있는 URL 링크들을 파싱해서 크롤링할 페이지들의 목록에 추가 해야 합니다. 크롤러들을 간단한 HTML 파싱을 해서 이들 링크들을 추출하고 상대 링크를 절대 링크로 변환할 필요가 있습니다.<br/>
<br/>

### 순환 피하기
로봇이 웹을 크롤링 할 때, `루프`나 `순환`에 빠지지 않도록 매우 조심해야 합니다. 그러기 위해 반드시 그들이 어디를 방문했는지 알아야 합니다. 그렇지 않으면 로봇을 함정에 빠뜨려 멈추게 하거나 진행을 느리게 하기 때문입니다. 그 이유는 **"루프와 중복"**에서 상세하게 알아보겠습니다.<br/>
<br/>

### 루프와 중복
순환은 아래의 세 가지 이유로 인해 크롤러에게 좋지 않습니다.
1. 크롤러를 루프에 빠뜨려 꼼짝 못하게 만들 수 있습니다. 
2. 같은 루프를 돌게 되면 고스란히 웹 서버의 부담이 됩니다. 
3. 중복된 페이지들을 가져오게 되면 중복된 컨텐츠로 넘쳐나게 됩니다.

<br/>

### 빵 부스러기의 흔적
셀 수도 없이 많은 URL은 빠른 검색 구조를 요구하기 때문에 **빠른 속도**는 중요합니다. 그러기 위해서 아래의 방법을 사용합니다.
- 트리와 해시 테이블 : 복잡한 로봇이라면 방문한 URL을 추적하기 위해 검색 트리나 해시 테이블을 사용했을 수도 있습니다.
- 느슨한 존재 비트맵 : 공간 사용을 최소화 하기 위해, 몇몇 대규모 크롤러들은 존재 비트 배열과 같은 느슨한 자료 구조를 사용합니다.
- 체크포인트 : 로봇 프로그램이 갑작스럽게 중단될 경우를 대비해서 방문한 URL의 목록이 디스크에 저장되었는지 확인합니다.
- 파티셔닝 : 웹이 성장하면서, 한 대의 컴퓨터에서 하나의 로봇이 크롤링을 완수하는 것을 불가능해져 각 로봇에 URL들의 특정 **한 부분**이 할당되어 그에 대한 책임을 집니다.

<br/>

### 별칭과 로봇 순환 
한 URL이 또 다른 URL에 대한 `별칭`이라면, 그 둘이 서로 달라 보이더라도 사실은 같은 리소스를 가리키고 있습니다.<br/>
<br/>

### URL 정규화하기
대부분의 웹 로봇은 URL들을 표준 형식으로 `정규화` 함으로써 다른 URL과 같은 리소스를 가지키고 있음이 확실한 것들을 미리 제거하려 시도합니다. 아래의 단계처럼 정규화된 형식으로 변환합니다.
1. 포트 번호가 명시되지 않았다면, 호스트 명에 :80 을 추가합니다.
2. 모든 %xx 이스케이핑된 문자들을 대응되는 문자로 변환합니다.
3. `#` 태그들을 제거합니다.

<br/>

### 파일 시스템 링크 순환
파일 시스템의 `심벌릭 링크`는 사실상 아무것도 존재하지 않으면서도 끝없이 깊어지는 디렉터리 계층을 만들 수 있기 때문에, 매우 교묘한 종류의 순환을 유발할 수 있습니다. 때로는 로봇을 함정에 빠뜨리기 위해 악의적으로 만들기도 합니다.<br/>
<br/>

### 동적 가상 웹 공간
웹 마스터가 나쁜 뜻이 없음에도 자신도 모르게 심벌릭 링크나 동적 컨텐츠를 통한 크롤러 함정을 만드는 것입니다.<br/>
<br/>

### 루프와 중복 피하기
루프와 중복을 피하는 완벽한 방법은 없지만 조금이라도 더 올바르게 작동하도록 하는 기법들이 있습니다.
- URL 정규화 : URL을 표준 형태로 변환함으로써, 같은 리소스를 가리키는 중복된 URL이 생기는 것을 일부 회피합니다.
- 너비 우선 크롤링 : 방문할 URL들을 웹 사이트들 전체에 걸쳐 너비 우선으로 스케줄링하면 순환의 영향을 최소화할 수 있습니다.
- 스로틀링 : 일정 시간 동안 가져올 수 있는 페이지의 숫자를 제한합니다.
- URL 크기 제한 : 일정 길이를 넘는 URL 의 크롤링을 거부 할 수 있습니다.
- URL/사이트 블랙리스트 : 문제를 일으키는 사이트나 URL이 발견될 때마다 블랙리스트에 추가합니다.
- 패턴 발견 : 파일시스템의 심벌리 링크를 통한 순환과 그와 비슷한 오설정들은 일정 패턴을 따르는 경향이 있어서 셋 이상의 반복되는 구성요소를 갖고 있는 URL을 크롤링하는 것을 거절합니다.
- 콘턴츠 지문 : 로봇이 이전에 보았던 체크섬을 가진 페이지를 가져온다면 해당 페이지는 크롤링 하지 않습니다.
- 사람 모니터링 : 더 작고, 더 커스터마이징된 크롤러들은 문제를 예방하기 위해 사람의 모니터링에 더욱 의존합니다.

<br/>

## 로봇의 HTTP
로봇 또한 **HTTP** 명세의 규칙을 지켜야 합니다.<br/>

### 요청 헤더 식별하기
로봇 대부분은 약간의 신원 식별 `헤더(특히 User-Agent HTTP 헤더)`를 구현하고 전송합니다. 로봇 구현자들은 로봇의 능력, 신원, 출신을 알려주는 기본적인 몇 가지 헤더를 사이트에게 보내주는 것이 좋습니다.
- User-Agent : 서버에게 요청을 만든 로봇의 이름을 말해줍니다.
- From : 로봇의 사용자/관리자의 이메일 주소를 제공합니다.
- Accept : 서버에게 어떤 미디어 타입을 보내도 되는지 말해줍니다.
- Referer : 현재의 요청 URL을 포함한 문서의 URL을 제공합니다.

<br/>

### 가상 호스팅
로봇 구현자들은 `Host 헤더`를 지원할 필요가 있습니다. 요청에 Host 헤더를 포함하지 않으면 로봇이 어떤 URL에 대해 잘못된 컨텐츠를 찾게 만들기 때문에 HTTP/1.1은 Host 헤더를 사용할 것을 요구합니다.<br/>
<br/>

### 조건부 요청
로봇이 검색하는 컨텐츠의 양을 **최소화**하는 것은 상당히 의미있는 일입니다. 예를들면 **변경되**었을 경우에만 컨텐츠를 가져오도록 하는 경우를 말합니다.<br/>
<br/>

### 응답 다루기
웹 탐색이나 서버와의 상호작용을 더 잘 해보려고 하는 로봇들은 여러 종류의 `HTTP 응답`을 다룰 줄 알 필요가 있습니다.
- 상태코드 : 모든 로봇은 최소한 일반적인 상태 코드나 예상할 수 있는 상태 코드를 다룰 수 있어야 합니다.
- 엔터티 : HTTP 헤더에 임베딩된 정보를 따라 로봇들은 엔터티 자체의 정보를 찾을 수 있습니다.

<br/>

### User-Agent 타게팅
웹 사이트들은 그들의 여러 기능을 지원할 수 있도록 브라우저의 종류를 감지하여 그에 맞게 컨텐츠를 **최적화**합니다. 이것을 함으로써, 사이트는 로봇에게 컨텐츠 대신 `에러 페이지`를 제공합니다. 사이트 관리자는 로봇의 요청을 다루기 위해서 `전략`을 세워 로봇이 방문하였을 때 컨텐츠가 없어 당황하는 일이 없도록 해야합니다.<br/>
<br/>

## 부적절하게 동작하는 로봇들
로봇이 저지르는 **실수** 몇 가지와 그로 인해 초래되는 **결과**를 몇 가지를 알아보겠습니다.
- 폭주하는 로봇 : 로봇은 사람보다 훨씬 빠르게 HTTP 요청을 만들 수 있습니다. 그리고 빠른 네트워크 연결을 갖춘 빠른 컴퓨터 위에서 동작합니다. 때문에 서버에 극심한 부하를 줄 수 있습니다.
- 오래된 URL : 만약 웹 사이트가 그들의 컨텐츠를 많이 바꾸었다면, 로봇들은 존재하지 않는 URL에 대한 요청을 보냅니다.
- 길고 잘못된 URL : 순환이나 프로그래밍상의 오류로 인해 로봇은 웹 사이트에게 크고 의미 없는 URL을 요청할 수 있습니다.
- 호기심이 지나친 로봇 : 사적인 데이터에 대한 URL을 얻어 데이터를 인터넷 검색엔진이나 애플리케이션을 통해 쉽게 접근할 수 있도록 만들 수도 있습니다.
- 동적 게이트웨이 접근 : 로봇은 게이트웨이 애플리케이션의 컨텐츠에 대한 URL로 요청을 할 수도 있습니다.

<br/>

## 로봇 차단하기
웹 서버는 서버의 문서 루트에 `robots.txt` 라고 이름 붙은 선택적인 파일을 제공함으로서 어떤 로봇이 서버의 어떤 부분에 **접근**할 수 있는지에 대한 **정보**가 담겨있습니다. robots.txt 파일은 로봇을 차단하지 않으므로 로봇은 그 페이지를 가져오게 됩니다.<br/>

### 로봇 차단 표준
`로봇 차단 표준`에는, 버전의 이름이 잘 정의되어 있지는 않지만 세 가지 버전이 있습니다.
- 0.0 : 로봇 배제 표준-Disallow 지시자를 지원하는 마틴 코스터의 오리지널 robots.txt 매커니즘 (1994.06)
- 1.0 : 웹 로봇 제어 방법-Allow 지시자의 지원이 추가된 마틴 소크터의 IETF 초안 (1996.11)
- 로봇 차단을 위한 확장 표준-정규식과 타이밍 정보를 포함한 숀 코너의 확장 (1996.11)

<br/>

### 웹 사이트와 robots.txt 파일들
웹 사이트의 어떤 URL을 방문하기전에, 웹 사이트에 robots.txt 파일이 **존재**한다면 로봇은 **반드시** 그 파일을 가져와야 합니다. robots.txt를 `가져오는 방법`은 아래와 같습니다.
- robots.txt 가져오기 : HTTP GET 메소드를 통해 robots.txt 리소스를 가져오고, 만약 파일이 존재한다면 서버는 그 파일은 text/plain 본문으로 반환합니다. 만약 서버가 404 Not Found HTTP 상태 코드로 응답한다면 로봇의 접근을 제한하지 않는 것으로 간주하고 어떤 파일이든 요청하게 될 것입니다.
- 응답코드:
  - 서버가 성공(200) 으로 응답하면 로봇은 응답의 컨테츠를 파싱하여 차단 규칙을 얻고, 그 사이트에서 무언가를 가져오려 할 때 그 규칙을 따라야 합니다.
  - 리소스가 존재하지 않는다고 서버가 응답하면(404) 규칙이 존재하지 않는다고 판단하고 robots.txt에 제약없이 사이트에 접근 할 수 있습니다.
  - 서버가 접근 제한(401 혹은 403)으로 응답한다면 접근은 완전히 제한되어 있다고 가정합니다.
  - 일시적으로 실패했다면(503) 리소스 검색을 뒤로 미룹니다.
  - 리다이렉션이면(3XX) 리소스가 발견될 때 까지 리다이렉트를 따라갑니다.

<br/>

### robots.txt 파일 포맷
robots.txt 파일의 각 줄은 **빈 줄, 주석 줄, 규칙 줄** 세 가지 종류가 있습니다. 규칙줄은 HTTP 헤더처럼 생겼고 (<필드>:<값>) 패턴 매칭을 위해 사용됩니다.
```
User-Agent: slurp
User-Agent: webcrawler
Disallow: /private

User-Agent: *
Disallow: 
```

- User-Agent 줄: 로봇의 레코드는 하나 이상의 User-Agent 줄로 시작합니다. 만약 로봇이 대응하는 User-Agent 줄을 찾지 못하였고 와일드 카드를 사용한 'User-Agent: *' 줄도 찾지 못했다면 접근에는 제한이 없습니다.
- Disallow 와 Allow 줄들: 이 줄들은 특정 로봇에 대해 어떤 URL 경로가 명시적으로 금지되어 있고 명시적으로 허용되는지 기술합니다.
- Disallow/Allow 접두매칭
  - Disallow나 Allow 규칙이 경로에 적용되려면, 그 경로의 시작부터 규칙 경로의 길이만큼의 문자열이 규칙 경로와 같아야한다.
  - 규칙 경로나 URL 경로의 임의의 '이스케이핑된' 문자들은 비교 전에 원래대로 복원됩니다.
  - 만약 어떤 규칙 경로가 빈 문자열이라면, 그 규칙은 모든 URL 경로와 같아야합니다.

<br/>

### 그 외에 알아둘 점
robots.txt 파일을 파싱할 때 지켜야 할 `규칙`이 몇 가지 더 있습니다.
- 명세 발전에 따라 User-Agnet, Disallow, Allow 외에 다른 필드를 포함할 수 있습니다. 그리고 자신이 이해 못하는 필드는 무시합니다.
- 하위 호환성을 한 줄을 여러 줄로 나누어 적는 것은 비허용합니다.
- 주석은 파일의 어디에서든 허용됩니다.
- 로봇 차단 표전 버전 0.0은 Allow 줄을 지원하지 않았습니다.

<br/>

### robots.txt 의 캐싱과 만료
매 요청마다 robots.txt 파일을 새로 가져와야 한다면 비효율적이기 때문에 주기적으로 robots.txt를 가져와서 그 결과를 `캐시`해야 합니다. 그렇기 때문에 로봇은 HTTP 응답의 `Cache-Controll`과 `Expires 헤더`의 주의를 기울여야 합니다.<br/>
<br/>

### 로봇 차단 펄 코드
robots.txt 파일과 상호작용하는 공개된 `펄 라이브러리`가 존재하여 WWW::RobotRules 객체로 여러 robots.txt 파일을 파싱할 수 있습니다.<br/>
<br/>

### HTML 로봇 제어 META 태그
로봇 차단 태그는 `HTML META 태그`를 이용해 다음과 같은 형식으로 구현됩니다.
```
<META NAME="ROBOTS" CONTENT="directive-list">
```

`로봇 META 지시자`에 대해서 알아보겠습니다.
- NOINDEX : 이 페이지를 처리하지 말고 무시하라고 전달합니다.
- NOFOLLOW : 이 페이지가 링크한 페이지를 크롤링하지 말라고 전달합니다.
- INDEX : 페이지의 컨텐츠를 인덱싱 허용합니다.
- FOLLOW : 이 페이지가 링크한 페이지를 크롤링하라고 전달합니다.
- NOARCHIVE : 이 페이지의 캐시를 위한 로컬 사본을 만들면 안 된다고 전달합니다.
- ALL : INDEX, FOLLOW와 같습니다.
- NONE : NOINDEX, NOFOLLOW와 같습니다.
- 검색엔진 META 태그
  - DESCRIPTION : 저자가 웹 페이지의 짧은 요약을 정의합니다.
  - KEYWORDS : 키워드 검색을 돕기 위해 쉼표로 구별합니다.
  - REVISIT-AFTER : 쉽게 변경될 페이지들을 지정된 만큼의 날짜가 지난 이후에 다시 방문해야 한다고 지시합니다.

<br/>

## 로봇 에티켓
1993년, 웹 로봇 커무니티의 개척자인 마틴 코스터는 웹 로봇을 만드는 사람들을 위한 `가이드라인 목록`을 작성하였습니다. 이 중 몇 가지는 구식이지만 대다수는 아직도 상당히 유용합니다.<br/><br/>

## 검색 엔진
웹 로봇을 가장 관범위하게 사용하는 것은 `인터넷 검색엔진`입니다. 이런 검색엔진이 어떻게 동작하는지 간략하게 알아보겠습니다.<br/>
### 넓게 생각하라
웹에서 수십억 개의 페이지들을 접근하는 오늘날, 인터넷 사용자들의 정보 찾기를 도와주는 검색엔진들은 필수가 되었습니다. 그들은 또한 급격히 성장하는 웹을 다루기 위해 진화하면서 많이 **복잡**해졌습니다. 그렇기 때문에 그만큼 복잡한 크롤러를 사용해야 합니다.<br/>
<br/>

### 현대적인 검색엔진의 아키텍쳐
오늘날 검색엔진들은 그들이 갖고 있는 전 세계의 웹페이지들에 대해 '풀 텍스트 색인'이라고 하는 복잡한 로컬 데이터베이스를 생성합니다. 이 색인은 웹의 모든 문서에 대한 일종의 카드 카탈로그처럼 동작합니다.<br/>
<br/>

### 풀 텍스트 색인
`풀 텍스트 색인`은 단어 하나를 입력받아 그 단어를 포함하고 있는 문서를 즉각 알려줄 수 있는 데이터베이스입니다. 이 문서들은 색인이 생성된 후에는 검색할 필요가 없습니다.<br/>
<br/>

### 질의 보내기
사용자가 질의를 웹 검색엔진 `게이트웨이`로 보내는 방법은, HTML 폼을 사용자가 채워 넣고 브라우저가 그 폼을 HTTP GET이나 POST 요청을 이용해서 게이트웨이로 보내는 방식입니다. 게이트웨이 프로그램은 검색 질의를 추출하고 웹 UI 질의를 풀 텍스트 색인을 검색할 때 사용되는 표현식으로 변환합니다.<br/>
<br/>

### 검색 결과를 정렬하고 보여주기
질의의 결과를 확인하기 위해 검색엔진이 색인을 한번 사용했다면, 게이트웨이 애플리케이션은 그 결과를 이용해 최종 사용자를 위한 결과 페이지를 즉석에서 만들어냅니다.<br/>
<br/>

### 스푸핑
사용자들은 자신이 찾는 내용이 검색 결과의 최상의 몇 줄에서 보이지 않는다면 불만족스러워 하기 때문에, 검색 결과의 순서는 매우 중요합니다. 이는 관련 알고리즘의 발전을 불러일으켰습니다.<br/>
<br/>